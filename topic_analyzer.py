import os
import time  # ‚Üê –í–û–¢ –≠–¢–û –ë–´–õ–û –ü–†–û–ü–£–©–ï–ù–û
import requests
from pathlib import Path
from navec import Navec
import numpy as np
from sklearn.cluster import DBSCAN
from collections import Counter

# URL —Å –º–æ–¥–µ–ª—å—é (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–µ—Ä–∫–∞–ª–æ –∏–ª–∏ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫)
MODEL_URL = "https://github.com/natasha/navec/releases/download/v1.0/navec_hudlit_v1_12B_500K_300d_100q.tar"
MODEL_FILENAME = "navec_hudlit_v1_12B_500K_300d_100q.tar"

def download_model():
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç, —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    if os.path.exists(MODEL_FILENAME):
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å: {MODEL_FILENAME}")
        return True
    
    print(f"üîÑ –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å (300 –ú–ë) —Å GitHub...")
    
    # –ü—Ä–æ–±—É–µ–º –¥–æ 3 —Ä–∞–∑
    for attempt in range(1, 4):
        try:
            response = requests.get(MODEL_URL, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(MODEL_FILENAME, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10%
                    if total_size > 0:
                        percent = int(100 * downloaded / total_size)
                        if percent % 10 == 0 and downloaded == int(total_size * percent / 100):
                            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {percent}%")
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞!")
            return True
            
        except Exception as e:
            print(f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ {attempt} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if attempt < 3:
                print(f"   –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)  # ‚Üê –¢–ï–ü–ï–†–¨ –†–ê–ë–û–¢–ê–ï–¢, –¢–ê–ö –ö–ê–ö time –ò–ú–ü–û–†–¢–ò–†–û–í–ê–ù
    
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫")
    return False

# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
if not download_model():
    print("‚ö†Ô∏è –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä!")
    USE_SIMPLE = True
else:
    USE_SIMPLE = False
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å Navec –≤ –ø–∞–º—è—Ç—å...")
    navec = Navec.load(MODEL_FILENAME)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

# –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
TOPIC_KEYWORDS = {
    '–æ—á–µ—Ä–µ–¥—å': ['–æ—á–µ—Ä–µ–¥', '–¥–æ–ª–≥–æ', '–∂–¥–∞—Ç—å', '—Å–∫–æ—Ä–æ—Å—Ç—å', '–±—ã—Å—Ç—Ä–æ', '–º–µ–¥–ª–µ–Ω–Ω–æ'],
    '–ø–µ—Ä—Å–æ–Ω–∞–ª': ['—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–≤–µ–∂–ª–∏–≤', '–≥—Ä—É–±', '—Ö–∞–º', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä', '–¥–µ–≤—É—à–∫–∞', '–ø–∞—Ä–µ–Ω—å'],
    '—á–∏—Å—Ç–æ—Ç–∞': ['—á–∏—Å—Ç', '–≥—Ä—è–∑', '—É–±—Ä–∞–Ω', '—Å–≤–µ—Ç–ª', '—Ç–µ–º–Ω', '–æ–ø—Ä—è—Ç–Ω'],
    '—Ü–µ–Ω—ã': ['—Ü–µ–Ω', '–¥–æ—Ä–æ–≥', '–¥–µ—à–µ–≤', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∫–æ–ø–µ–π–∫'],
    '–∫–∞—á–µ—Å—Ç–≤–æ': ['–∫–∞—á–µ—Å—Ç–≤', '—Ç–æ–≤–∞—Ä', '–ø—Ä–æ–¥—É–∫—Ç', '–±—Ä–∞–∫', '—Å–ª–æ–º–∞–Ω'],
    '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫', '–∫—É—Ä—å–µ—Ä', '–ø—Ä–∏–≤–µ–∑', '–æ–ø–æ–∑–¥–∞–Ω'],
    '–ø–∞—Ä–∫–æ–≤–∫–∞': ['–ø–∞—Ä–∫–æ–≤–∫', '–º–∞—à–∏–Ω', '–º–µ—Å—Ç–æ', '–ø—Ä–∏–ø–∞—Ä–∫–æ–≤–∞—Ç—å—Å—è'],
    '–∞—Ç–º–æ—Å—Ñ–µ—Ä–∞': ['–∞—Ç–º–æ—Å—Ñ–µ—Ä', '—É—é—Ç–Ω', '–∫–æ–º—Ñ–æ—Ä—Ç–Ω', '–º—É–∑—ã–∫']
}

def simple_topic_analyzer(text):
    """–ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å"""
    text_lower = text.lower()
    topic_scores = {}
    
    for topic, keywords in TOPIC_KEYWORDS.items():
        score = sum(1 for keyword in keywords if keyword in text_lower)
        if score > 0:
            topic_scores[topic] = score
    
    if not topic_scores:
        return "—Ä–∞–∑–Ω–æ–µ"
    
    return max(topic_scores, key=topic_scores.get)

def text_to_vector(text):
    """Navec version"""
    if USE_SIMPLE:
        return None
    
    words = text.lower().split()
    vectors = []
    for word in words:
        if word in navec:
            vectors.append(navec[word])
    if not vectors:
        return None
    return np.mean(vectors, axis=0)

def get_topic_from_cluster(cluster_texts):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—É –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    all_words = []
    for text in cluster_texts:
        words = text.lower().split()
        all_words.extend(words)
    
    stopwords = {'–æ—á–µ–Ω—å', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–µ', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–∫–∞–∫', '—É', '–≤—Å–µ'}
    filtered = [w for w in all_words if w not in stopwords and len(w) > 2]
    
    if not filtered:
        return "—Ä–∞–∑–Ω–æ–µ"
    
    return Counter(filtered).most_common(1)[0][0]

class TopicClassifier:
    def __init__(self, eps=0.5, min_samples=2):
        self.eps = eps
        self.min_samples = min_samples
        self.use_navec = not USE_SIMPLE
    
    def predict(self, new_review_text, all_recent_reviews=None):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—É –æ—Ç–∑—ã–≤–∞"""
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        if not self.use_navec:
            return simple_topic_analyzer(new_review_text)
        
        # Navec version
        if not all_recent_reviews or len(all_recent_reviews) < 2:
            return simple_topic_analyzer(new_review_text)
        
        texts = [new_review_text] + all_recent_reviews
        vectors = []
        valid_texts = []
        
        for t in texts:
            vec = text_to_vector(t)
            if vec is not None:
                vectors.append(vec)
                valid_texts.append(t)
        
        if len(vectors) < 2:
            return simple_topic_analyzer(new_review_text)
        
        clustering = DBSCAN(eps=self.eps, min_samples=self.min_samples, metric='cosine').fit(vectors)
        
        new_vec = text_to_vector(new_review_text)
        if new_vec is None:
            return simple_topic_analyzer(new_review_text)
        
        label = clustering.labels_[0]
        if label == -1:
            return simple_topic_analyzer(new_review_text)
        
        cluster_texts = [valid_texts[i] for i, lbl in enumerate(clustering.labels_) if lbl == label]
        return get_topic_from_cluster(cluster_texts)